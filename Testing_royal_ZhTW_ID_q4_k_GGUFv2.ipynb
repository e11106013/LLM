{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/e11106013/LLM/blob/main/Testing_royal_ZhTW_ID_q4_k_GGUFv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 安裝軟體套件\n",
        "!pip install llama-cpp-python gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYj6IOOWbzzc",
        "outputId": "c9c3ecc1-6d2a-4639-e570-d9195e5cd2e6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.32.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.10.2 (from gradio)\n",
            "  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.4)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.32.1-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.12-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4067779 sha256=db00c63a33cbb6a52f332c975d22f46b2bd93c6391b472cabfaa081dcaf4ddd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, diskcache, aiofiles, starlette, llama-cpp-python, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 diskcache-5.6.3 fastapi-0.115.12 ffmpy-0.6.0 gradio-5.32.1 gradio-client-1.10.2 groovy-0.1.2 llama-cpp-python-0.3.9 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.12 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU2IftMWcx6f",
        "outputId": "c7f66e21-f905-4399-8694-b28b436b51c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun  3 15:44:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0cWkgpbdvqS",
        "outputId": "f2c72127-bc35-4edf-eb6d-48dac38db13a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 登入HF\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SvXXAhEZiq2",
        "outputId": "081323cf-5a60-48e5-ff3a-c2d9d4af97d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `test` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `test`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 下載自HF下載模型\n",
        "!mkdir -p /content/models\n",
        "!huggingface-cli download roylin1003/royal-ZhTW-ID-q4_k_m --local-dir models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2FIt1HLZiZ3",
        "outputId": "70d245a5-50cb-48e1-dc37-61bb8e96e0dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 3 files:   0% 0/3 [00:00<?, ?it/s]Downloading '.gitattributes' to 'models/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.b2080b8d46450ae2b28f6c66dd0043175d74b359.incomplete'\n",
            "Downloading 'README.md' to 'models/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.92188540f75c6de7cceea5d162263354e55535de.incomplete'\n",
            "Downloading 'royal-ZhTW-ID-q4_k_m.gguf' to 'models/.cache/huggingface/download/mEEja2CYA9yEXr8ptZUooI78S7M=.eb71259df38f1ae46e6dd44d3b0700820c733a995d0ada1e95ca4eb7a9f2f83c.incomplete'\n",
            "\n",
            "\r.gitattributes:   0% 0.00/1.58k [00:00<?, ?B/s]\u001b[A\r.gitattributes: 100% 1.58k/1.58k [00:00<00:00, 8.28MB/s]\n",
            "Download complete. Moving file to models/.gitattributes\n",
            "\rFetching 3 files:  33% 1/3 [00:00<00:00,  7.44it/s]\n",
            "\rREADME.md:   0% 0.00/2.94k [00:00<?, ?B/s]\u001b[A\rREADME.md: 100% 2.94k/2.94k [00:00<00:00, 15.6MB/s]\n",
            "Download complete. Moving file to models/README.md\n",
            "\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   0% 0.00/4.68G [00:00<?, ?B/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   0% 10.5M/4.68G [00:00<00:56, 82.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   1% 31.5M/4.68G [00:00<00:36, 129MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   1% 52.4M/4.68G [00:00<00:31, 146MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   2% 73.4M/4.68G [00:00<00:28, 161MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   2% 94.4M/4.68G [00:00<00:27, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   3% 126M/4.68G [00:00<00:24, 186MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   3% 147M/4.68G [00:00<00:24, 186MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   4% 168M/4.68G [00:00<00:24, 185MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   4% 199M/4.68G [00:01<00:22, 197MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   5% 220M/4.68G [00:01<00:22, 197MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   5% 252M/4.68G [00:01<00:21, 204MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   6% 273M/4.68G [00:01<00:21, 204MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   6% 294M/4.68G [00:01<00:21, 204MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   7% 315M/4.68G [00:01<00:22, 196MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   7% 336M/4.68G [00:01<00:22, 194MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   8% 367M/4.68G [00:01<00:21, 202MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   9% 398M/4.68G [00:02<00:20, 206MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:   9% 430M/4.68G [00:02<00:20, 209MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  10% 451M/4.68G [00:02<00:20, 206MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  10% 472M/4.68G [00:02<00:21, 200MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  11% 503M/4.68G [00:02<00:20, 202MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  11% 535M/4.68G [00:02<00:20, 207MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  12% 556M/4.68G [00:02<00:20, 206MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  12% 577M/4.68G [00:02<00:19, 206MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  13% 598M/4.68G [00:03<00:19, 205MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  13% 619M/4.68G [00:03<00:20, 202MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  14% 640M/4.68G [00:03<00:19, 204MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  14% 661M/4.68G [00:03<00:20, 194MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  15% 682M/4.68G [00:03<00:20, 191MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  15% 703M/4.68G [00:03<00:21, 187MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  15% 724M/4.68G [00:03<00:21, 186MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  16% 744M/4.68G [00:03<00:21, 185MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  16% 765M/4.68G [00:03<00:21, 178MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  17% 786M/4.68G [00:04<00:21, 177MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  17% 807M/4.68G [00:04<00:22, 174MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  18% 828M/4.68G [00:04<00:21, 177MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  18% 849M/4.68G [00:04<00:22, 174MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  19% 870M/4.68G [00:04<00:22, 172MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  19% 891M/4.68G [00:07<02:45, 23.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  19% 912M/4.68G [00:07<02:01, 31.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  20% 933M/4.68G [00:07<01:30, 41.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  20% 954M/4.68G [00:07<01:08, 54.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  21% 975M/4.68G [00:07<00:54, 67.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  21% 996M/4.68G [00:07<00:44, 83.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  22% 1.02G/4.68G [00:08<00:37, 97.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  22% 1.04G/4.68G [00:08<00:33, 110MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  23% 1.06G/4.68G [00:08<00:30, 120MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  23% 1.08G/4.68G [00:08<00:27, 129MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  24% 1.10G/4.68G [00:08<00:25, 141MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  24% 1.12G/4.68G [00:08<00:23, 150MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  24% 1.14G/4.68G [00:08<00:22, 157MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  25% 1.16G/4.68G [00:08<00:21, 164MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  25% 1.18G/4.68G [00:09<00:20, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  26% 1.21G/4.68G [00:09<00:20, 173MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  26% 1.23G/4.68G [00:09<00:20, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  27% 1.25G/4.68G [00:09<00:20, 167MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  27% 1.27G/4.68G [00:09<00:20, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  28% 1.29G/4.68G [00:09<00:20, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  28% 1.31G/4.68G [00:09<00:20, 166MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  28% 1.33G/4.68G [00:13<03:24, 16.4MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  29% 1.35G/4.68G [00:13<02:28, 22.4MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  29% 1.37G/4.68G [00:14<01:48, 30.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  30% 1.39G/4.68G [00:14<01:21, 40.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  30% 1.42G/4.68G [00:14<01:01, 52.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  31% 1.44G/4.68G [00:14<00:48, 66.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  31% 1.46G/4.68G [00:14<00:39, 81.4MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  32% 1.48G/4.68G [00:14<00:33, 96.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  32% 1.50G/4.68G [00:14<00:28, 112MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  32% 1.52G/4.68G [00:14<00:25, 125MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  33% 1.54G/4.68G [00:14<00:23, 136MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  33% 1.56G/4.68G [00:15<00:21, 147MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  34% 1.58G/4.68G [00:15<00:19, 155MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  34% 1.60G/4.68G [00:15<00:19, 160MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  35% 1.63G/4.68G [00:15<00:18, 164MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  35% 1.65G/4.68G [00:15<00:18, 167MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  36% 1.67G/4.68G [00:15<00:18, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  36% 1.69G/4.68G [00:15<00:18, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  36% 1.71G/4.68G [00:15<00:18, 163MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  37% 1.73G/4.68G [00:16<00:17, 167MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  37% 1.75G/4.68G [00:16<00:26, 111MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  38% 1.77G/4.68G [00:16<00:23, 122MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  38% 1.79G/4.68G [00:16<00:21, 133MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  39% 1.81G/4.68G [00:16<00:20, 142MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  39% 1.84G/4.68G [00:16<00:18, 151MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  40% 1.86G/4.68G [00:17<00:17, 158MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  40% 1.88G/4.68G [00:17<00:17, 162MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  41% 1.90G/4.68G [00:17<00:16, 164MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  41% 1.92G/4.68G [00:17<00:16, 166MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  41% 1.94G/4.68G [00:17<00:16, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  42% 1.96G/4.68G [00:17<00:15, 173MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  42% 1.98G/4.68G [00:17<00:15, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  43% 2.00G/4.68G [00:17<00:15, 173MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  43% 2.02G/4.68G [00:18<00:15, 172MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  44% 2.04G/4.68G [00:18<00:15, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  44% 2.07G/4.68G [00:18<00:15, 172MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  45% 2.09G/4.68G [00:18<00:15, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  45% 2.11G/4.68G [00:18<00:15, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  45% 2.13G/4.68G [00:18<00:15, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  46% 2.15G/4.68G [00:18<00:15, 167MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  46% 2.17G/4.68G [00:18<00:14, 172MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  47% 2.19G/4.68G [00:19<00:14, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  47% 2.21G/4.68G [00:19<00:14, 174MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  48% 2.23G/4.68G [00:19<00:13, 177MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  48% 2.25G/4.68G [00:19<00:13, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  49% 2.28G/4.68G [00:19<00:13, 174MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  49% 2.30G/4.68G [00:19<00:13, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  49% 2.32G/4.68G [00:19<00:13, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  50% 2.34G/4.68G [00:19<00:13, 172MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  50% 2.36G/4.68G [00:19<00:13, 171MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  51% 2.38G/4.68G [00:21<00:59, 38.4MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  51% 2.40G/4.68G [00:21<00:45, 49.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  52% 2.42G/4.68G [00:21<00:35, 63.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  52% 2.44G/4.68G [00:21<00:28, 78.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  53% 2.46G/4.68G [00:22<00:23, 94.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  53% 2.49G/4.68G [00:22<00:20, 108MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  54% 2.51G/4.68G [00:22<00:18, 117MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  54% 2.53G/4.68G [00:22<00:17, 125MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  54% 2.55G/4.68G [00:22<00:16, 127MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  55% 2.57G/4.68G [00:22<00:16, 129MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  55% 2.59G/4.68G [00:22<00:15, 132MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  56% 2.61G/4.68G [00:23<00:15, 138MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  56% 2.63G/4.68G [00:23<00:14, 145MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  57% 2.65G/4.68G [00:23<00:14, 145MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  57% 2.67G/4.68G [00:23<00:13, 151MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  58% 2.69G/4.68G [00:23<00:13, 150MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  58% 2.72G/4.68G [00:23<00:13, 149MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  58% 2.74G/4.68G [00:23<00:15, 129MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  59% 2.76G/4.68G [00:24<00:18, 103MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  59% 2.78G/4.68G [00:24<00:22, 84.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  60% 2.79G/4.68G [00:24<00:24, 76.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  60% 2.81G/4.68G [00:24<00:21, 89.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  60% 2.82G/4.68G [00:25<00:20, 88.7MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  60% 2.83G/4.68G [00:25<00:24, 75.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  61% 2.85G/4.68G [00:25<00:22, 80.3MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  61% 2.86G/4.68G [00:25<00:22, 80.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  61% 2.87G/4.68G [00:25<00:23, 78.3MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  62% 2.88G/4.68G [00:25<00:23, 75.3MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  62% 2.89G/4.68G [00:26<00:22, 81.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  62% 2.90G/4.68G [00:26<00:21, 80.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  62% 2.92G/4.68G [00:26<00:22, 79.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  62% 2.93G/4.68G [00:26<00:22, 78.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  63% 2.94G/4.68G [00:26<00:21, 79.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  63% 2.96G/4.68G [00:26<00:16, 107MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  64% 2.98G/4.68G [00:26<00:13, 124MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  64% 3.00G/4.68G [00:26<00:12, 140MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  64% 3.02G/4.68G [00:27<00:11, 148MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  65% 3.04G/4.68G [00:27<00:10, 151MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  65% 3.06G/4.68G [00:27<00:10, 150MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  66% 3.08G/4.68G [00:27<00:10, 155MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  66% 3.10G/4.68G [00:27<00:09, 162MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  67% 3.12G/4.68G [00:27<00:09, 162MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  67% 3.15G/4.68G [00:27<00:09, 163MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  68% 3.17G/4.68G [00:27<00:09, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  68% 3.19G/4.68G [00:28<00:08, 169MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  69% 3.21G/4.68G [00:28<00:08, 169MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  69% 3.23G/4.68G [00:28<00:08, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  69% 3.25G/4.68G [00:28<00:08, 175MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  70% 3.27G/4.68G [00:28<00:08, 173MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  70% 3.29G/4.68G [00:28<00:08, 170MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  71% 3.31G/4.68G [00:28<00:08, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  71% 3.33G/4.68G [00:28<00:07, 171MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  72% 3.36G/4.68G [00:29<00:12, 110MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  72% 3.38G/4.68G [00:29<00:10, 122MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  73% 3.40G/4.68G [00:29<00:09, 131MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  73% 3.42G/4.68G [00:31<00:51, 24.6MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  73% 3.44G/4.68G [00:32<00:37, 33.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  74% 3.46G/4.68G [00:32<00:27, 43.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  74% 3.48G/4.68G [00:32<00:21, 56.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  75% 3.50G/4.68G [00:32<00:16, 71.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  75% 3.52G/4.68G [00:32<00:13, 86.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  76% 3.54G/4.68G [00:32<00:11, 99.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  76% 3.57G/4.68G [00:32<00:09, 114MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  77% 3.59G/4.68G [00:32<00:08, 125MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  77% 3.61G/4.68G [00:33<00:07, 136MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  77% 3.63G/4.68G [00:34<00:26, 40.0MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  78% 3.65G/4.68G [00:34<00:20, 49.3MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  78% 3.67G/4.68G [00:34<00:16, 62.3MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  79% 3.69G/4.68G [00:34<00:12, 76.9MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  79% 3.71G/4.68G [00:35<00:10, 92.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  80% 3.73G/4.68G [00:35<00:08, 108MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  80% 3.75G/4.68G [00:35<00:08, 113MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  81% 3.77G/4.68G [00:35<00:07, 118MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  81% 3.80G/4.68G [00:35<00:07, 122MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  82% 3.82G/4.68G [00:35<00:06, 126MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  82% 3.84G/4.68G [00:35<00:06, 129MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  82% 3.86G/4.68G [00:36<00:06, 136MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  83% 3.88G/4.68G [00:36<00:05, 142MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  83% 3.90G/4.68G [00:36<00:05, 145MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  84% 3.92G/4.68G [00:36<00:05, 149MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  84% 3.94G/4.68G [00:36<00:04, 155MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  85% 3.96G/4.68G [00:36<00:05, 127MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  85% 3.98G/4.68G [00:37<00:06, 112MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  86% 4.01G/4.68G [00:37<00:05, 117MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  86% 4.03G/4.68G [00:37<00:05, 129MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  86% 4.05G/4.68G [00:37<00:05, 109MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  87% 4.07G/4.68G [00:37<00:05, 120MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  87% 4.09G/4.68G [00:38<00:05, 102MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  88% 4.11G/4.68G [00:38<00:05, 109MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  88% 4.13G/4.68G [00:38<00:05, 110MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  89% 4.15G/4.68G [00:38<00:05, 106MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  89% 4.17G/4.68G [00:38<00:05, 96.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  89% 4.18G/4.68G [00:38<00:05, 96.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  90% 4.20G/4.68G [00:39<00:04, 113MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  90% 4.23G/4.68G [00:39<00:03, 126MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  91% 4.25G/4.68G [00:42<00:22, 19.7MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  91% 4.27G/4.68G [00:42<00:15, 27.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  92% 4.29G/4.68G [00:42<00:10, 36.8MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  92% 4.31G/4.68G [00:42<00:07, 48.5MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  92% 4.33G/4.68G [00:42<00:05, 62.2MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  93% 4.35G/4.68G [00:42<00:04, 76.6MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  93% 4.37G/4.68G [00:42<00:03, 91.1MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  94% 4.39G/4.68G [00:43<00:02, 107MB/s] \u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  94% 4.41G/4.68G [00:43<00:02, 122MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  95% 4.44G/4.68G [00:43<00:01, 134MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  95% 4.46G/4.68G [00:43<00:01, 141MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  96% 4.48G/4.68G [00:43<00:01, 146MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  96% 4.50G/4.68G [00:43<00:01, 153MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  97% 4.52G/4.68G [00:43<00:01, 153MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  97% 4.54G/4.68G [00:43<00:00, 158MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  97% 4.56G/4.68G [00:44<00:00, 160MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  98% 4.58G/4.68G [00:44<00:00, 166MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  98% 4.60G/4.68G [00:44<00:00, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  99% 4.62G/4.68G [00:44<00:00, 168MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf:  99% 4.65G/4.68G [00:44<00:00, 166MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf: 100% 4.67G/4.68G [00:44<00:00, 165MB/s]\u001b[A\n",
            "royal-ZhTW-ID-q4_k_m.gguf: 100% 4.68G/4.68G [00:44<00:00, 104MB/s]\n",
            "Download complete. Moving file to models/royal-ZhTW-ID-q4_k_m.gguf\n",
            "Fetching 3 files: 100% 3/3 [00:45<00:00, 15.01s/it]\n",
            "/content/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# --- 載入模型（使用 cache 避免重複載入） ---\n",
        "def load_model():\n",
        "    return Llama(\n",
        "        model_path=\"models/royal-ZhTW-ID-q4_k_m.gguf\",\n",
        "      #  n_gpu_layers=20,\n",
        "        n_gpu_layers=-1,    # 使用所有 GPU 層（-1 表示全部）\n",
        "\n",
        "        n_ctx=2048\n",
        "    )\n",
        "\n",
        "llm = load_model()\n",
        "\n",
        "# --- 翻譯函式 ---\n",
        "def translate(input_text, direction, history):\n",
        "    if direction == \"華語 ➜ 印尼語\":\n",
        "        prompt = f\"\"\"請將下列華語句子精準翻譯為印尼語，只回傳印尼語內容，不要翻譯成其他語言：\n",
        "華語：{input_text.strip()}\n",
        "印尼語：\"\"\"\n",
        "        output_prefix = \"印尼語：\"\n",
        "    else:\n",
        "        prompt = f\"\"\"請將下列印尼語句子精準翻譯為華語，只回傳華語內容，不要翻譯成其他語言：\n",
        "印尼語：{input_text.strip()}\n",
        "華語：\"\"\"\n",
        "        output_prefix = \"華語：\"\n",
        "\n",
        "    # 模型推論\n",
        "    output = llm(prompt, max_tokens=100)\n",
        "    result = output[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "    user_display = f\"👤 {input_text.strip()}\"\n",
        "    bot_display = f\"🤖 {output_prefix}{result}\"\n",
        "\n",
        "    history.append((user_display, bot_display))\n",
        "    return \"\", history\n",
        "\n",
        "# --- Gradio 介面設定 ---\n",
        "with gr.Blocks(title=\"繁中印尼翻譯助理\") as demo:\n",
        "    gr.Markdown(\"# 🗣️ 繁中 ↔ 印尼 翻譯對話介面\")\n",
        "    gr.Markdown(\"使用模型：`roylin1003/royal-ZhTW-ID-q4_k_m`\")\n",
        "\n",
        "    direction = gr.Radio(\n",
        "        [\"華語 ➜ 印尼語\", \"印尼語 ➜ 華語\"],\n",
        "        label=\"請選擇翻譯方向\",\n",
        "        value=\"華語 ➜ 印尼語\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(placeholder=\"請輸入要翻譯的句子...\", label=\"輸入\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    msg.submit(translate, [msg, direction, state], [msg, chatbot])\n",
        "\n",
        "# --- 執行 ---\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "csZ5-jZHZYRp",
        "outputId": "a0e7525e-2265-49cd-c855-e2d11a55d731"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 339 tensors from models/royal-ZhTW-ID-q4_k_m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Qwen\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Qwen2.5\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
            "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 3584\n",
            "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 18944\n",
            "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 28\n",
            "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 4\n",
            "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  26:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.36 GiB (4.91 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 3584\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 28\n",
            "print_info: n_head_kv        = 4\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 7\n",
            "print_info: n_embd_k_gqa     = 512\n",
            "print_info: n_embd_v_gqa     = 512\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 18944\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.62 B\n",
            "print_info: general.name     = Qwen2.5 7B Instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 152064\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 170 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =  2976.75 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  4424.03 MiB\n",
            "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            "..................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
            "llama_context:        CPU compute buffer size =   304.00 MiB\n",
            "llama_context: graph nodes  = 1042\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2.5', 'qwen2.embedding_length': '3584', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2.5 7B Instruct', 'qwen2.block_count': '28', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Qwen', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '7B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '18944', 'qwen2.attention.head_count': '28'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "<ipython-input-6-d61c3c79ccaa>:50: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://67e0ef4f3dc2c625cc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://67e0ef4f3dc2c625cc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}